# -*- coding: utf-8 -*-
"""Dicoding_NLP_BBCNEWS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_2XdFrG4A-5rfOJ5zKgDiFXD-eqFD8f

#Dicoding NLP Submission
Nama : Immanuel Mayerd

##Membaca dan Memvisualisasikan persebaran Data
"""

#import library
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

#Install package kaggle
!pip install -q kaggle

#Membuat fungsi upload credential kaggle
def upload_kaggle_credentials():
    from google.colab import files
    uploaded = files.upload()

    !mkdir -p ~/.kaggle
    !cp kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
    !ls ~/.kaggle

#Mendownload dataset BBC News yang telah disimpan di akun kaggle pribadi
upload_kaggle_credentials()
!kaggle datasets download -d immanuelmayerd/bbc-news

# Mengekstrak dataset
!mkdir bbc-news
!unzip bbc-news.zip -d bbc-news
!ls bbc-news

#Membuat dataframe dengan nama df_tc
data_path = "/content/bbc-news/BBC News.csv"
df_tc = pd.read_csv(data_path, sep=",")

#Menampilkan head df_tc
df_tc.head()

#Mengecek ukuran dataset
df_tc.shape

#Mengecek apakah ada nilai Null
df_tc.isnull().sum()

#Mengecek memory usage
print(df_tc.info())

#Mengecek nilai unik yang akan dijadikan label
print(df_tc['category'].unique())

#Memvisualisasikan persebaran data berdasarkan category
sns.countplot(x=df_tc['category'])

"""##Preprocessing Data"""

#Membuat fungsi cleaning data dengan stopwords
def cleaning(text):
    text = text.lower().replace('\n',' ').replace('\r','').strip()
    text = re.sub(' +', ' ', text)
    text = re.sub(r'[^\w\s]','',text)

    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    filtered_sentence = []
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)

    text = " ".join(filtered_sentence)
    return text

#Membuat kolom baru berdasarkan raw data yang dibersihkan dengan fungsi cleaning
df_tc['Filtered'] = df_tc['text'].apply(cleaning)

#Mengecek data hasil cleaning
df_tc

"""##Enconding dan Finalisasi dataframe"""

#Melakukan Encoding label menjadi 0 dan 1
category = pd.get_dummies(df_tc['category'])
new_df = pd.concat([df_tc, category], axis=1)
new_df = new_df.drop(columns=['category','text'])
new_df.head(10)

#Mengecek shape dataframe baru
new_df.shape

#Memisahkan label dengan data yang akan dilatih
value = new_df['Filtered'].values
label = new_df[['business', 'entertainment', 'politics', 'sport', 'tech']].values

"""## Pemodelan Sequential dengan LSTM"""

#Membagi data train dan test menggunakan sklearn
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(value,
                                                    label,
                                                    test_size=0.2,
                                                    random_state=5)

#Import Library untuk NLP
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

#Melakukan Tokenizer
vocab_size = 10000
max_len = 200
trunc_type = "post"
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(x_train)
word_index = tokenizer.word_index
sequences_train = tokenizer.texts_to_sequences(x_train)
sequences_test = tokenizer.texts_to_sequences(x_test)
pad_train = pad_sequences(sequences_train, maxlen=max_len, truncating=trunc_type)
pad_test = pad_sequences(sequences_test, maxlen=max_len, truncating=trunc_type)

print(pad_test.shape)

#Mengecek bentuk akhir data
pad_train

#Mengecek bentuk akhir data
pad_test

#Membangun model sequential dengan embedding dan LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

#Membuat funsi callback berdasarkan akurasi
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.90):
      self.model.stop_training = True
      print("\n Akurasi telah terpenuhi!")
callbacks = myCallback()

#Melakukan latih model yang disimpan dalam variabel history
num_epochs = 25
history = model.fit(pad_train, y_train, epochs=num_epochs,
                    validation_data=(pad_test, y_test), verbose=2, callbacks=[callbacks])

"""##Plot akurasi dan loss hasil Pelatihan"""

# plot of accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# plot of loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()